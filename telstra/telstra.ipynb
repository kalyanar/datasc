{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TreeStruct and RefinedRandomClassifier copied from dune_dweller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class TreeStruct():\n",
    "    TREE_LEAF = -1\n",
    "    DELETED_LEAF = -5\n",
    "\n",
    "    def __init__(self, tree):\n",
    "        self.children_left = tree.children_left\n",
    "        self.children_right = tree.children_right\n",
    "        self.value = tree.value\n",
    "        self.update_leaves()\n",
    "\n",
    "    def update_leaves(self):\n",
    "        self.leaves = np.nonzero(self.children_left==TreeStruct.TREE_LEAF)[0]\n",
    "        if self.leaves.shape[0] == 1:\n",
    "            # this tree has been pruned to the root, we should delete it from the list of estimators\n",
    "            return True\n",
    "        self.leaf_siblings = np.array([self.find_sibling_node(leaf) for leaf in self.leaves])\n",
    "        self.leaf_pos = np.zeros(self.children_left.shape[0],dtype=np.int32) - 1\n",
    "        positions = np.arange(self.leaves.shape[0])\n",
    "        self.leaf_pos[self.leaves] = positions\n",
    "        return False\n",
    "\n",
    "    def find_sibling_node(self, node):\n",
    "        left = np.nonzero((self.children_left==node))[0]\n",
    "        if left.shape[0] > 0:\n",
    "            return self.children_right[left[0]]\n",
    "        right = np.nonzero((self.children_right==node))[0]\n",
    "        return self.children_left[right[0]]\n",
    "\n",
    "    def is_leaf(self, node):\n",
    "        return self.children_left[node] == TreeStruct.TREE_LEAF\n",
    "    def is_pruned(self, node):\n",
    "        return self.children_left[node] == TreeStruct.DELETED_LEAF\n",
    "\n",
    "    def sibling_leaf_positions(self):\n",
    "        return self.leaf_pos[self.leaf_siblings]\n",
    "\n",
    "    def merge_leaves(self, leaf):\n",
    "        if self.is_pruned(leaf):\n",
    "            return False # already merged\n",
    "        else:\n",
    "            assert self.is_leaf(leaf)\n",
    "            sib = self.leaf_siblings[self.leaf_pos[leaf]]\n",
    "            if not self.is_leaf(sib):\n",
    "                return False # can't merge leaf with branch\n",
    "            self.children_left[[leaf, sib]] = TreeStruct.DELETED_LEAF\n",
    "            self.children_right[[leaf, sib]] =  TreeStruct.DELETED_LEAF\n",
    "            parent = np.nonzero(np.logical_or(self.children_left==leaf,self.children_right == leaf))[0][0]\n",
    "            self.children_left[parent] = TreeStruct.TREE_LEAF\n",
    "            self.children_right[parent] = TreeStruct.TREE_LEAF\n",
    "            return True\n",
    "\n",
    "\n",
    "class RefinedRandomForest():\n",
    "    def __init__(self, rf, C = 1.0, prune_pct = 0.1, n_prunings = 1, criterion = 'sumnorm'):\n",
    "        self.rf_ = rf\n",
    "        self.C = C\n",
    "        self.prune_pct = prune_pct\n",
    "        self.n_prunings = n_prunings\n",
    "        self.criterion = criterion\n",
    "        self.trees_ = [TreeStruct(tree.tree_) for tree in rf.estimators_]\n",
    "        self.leaves()\n",
    "    \n",
    "    def leaves(self):\n",
    "        self.n_leaves_ = [tree.leaves.shape[0] for tree in self.trees_]\n",
    "        self.M = np.sum(self.n_leaves_)\n",
    "        self.offsets_ = np.zeros_like(self.n_leaves_)\n",
    "        self.offsets_[1:] = np.cumsum(self.n_leaves_)[:-1]\n",
    "        self.ind_trees_ = np.zeros(self.M,dtype=np.int32)\n",
    "        self.ind_leaves_ = np.zeros(self.M,dtype=np.int32)\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            start = self.offsets_[tree_ind]\n",
    "            end = self.offsets_[tree_ind+1] if tree_ind+1<len(self.trees_) else self.M\n",
    "            self.ind_trees_[start:end] = tree_ind\n",
    "            self.ind_leaves_[start:end] = tree.leaves\n",
    "\n",
    "    def get_indicators(self, X):\n",
    "        leaf = self.rf_.apply(X)\n",
    "        sample_ind = np.arange(X.shape[0])\n",
    "        row_ind = []\n",
    "        col_ind = []\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            X_leaves = leaf[:,tree_ind]\n",
    "            row_ind.append(sample_ind)\n",
    "            col_ind.append(self.offsets_[tree_ind]+tree.leaf_pos[X_leaves])\n",
    "        row_ind = np.concatenate(row_ind)\n",
    "        col_ind = np.concatenate(col_ind)\n",
    "        data = np.ones_like(row_ind)\n",
    "        indicators = csr_matrix((data, (row_ind, col_ind)), shape=(X.shape[0],self.M))\n",
    "        return indicators\n",
    "\n",
    "    def prune_trees(self):\n",
    "        ind_siblings = np.zeros_like(self.ind_leaves_)\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            offset = self.offsets_[tree_ind]\n",
    "            sibl_ind = tree.sibling_leaf_positions()\n",
    "            sibl_ind[sibl_ind>=0] += offset\n",
    "            start = self.offsets_[tree_ind]\n",
    "            end = self.offsets_[tree_ind+1] if tree_ind+1<len(self.trees_) else self.M\n",
    "            ind_siblings[start:end] = sibl_ind\n",
    "        coef = self.lr.coef_\n",
    "        sibl_coef = coef[:,ind_siblings]\n",
    "        sibl_coef[:,ind_siblings < 0] = np.inf # so that we don't merge leaf with branch\n",
    "        if self.criterion == 'sumnorm':\n",
    "            sum_coef = np.sum(coef**2 + sibl_coef**2,axis=0)\n",
    "        elif self.criterion == 'normdiff':\n",
    "            sum_coef = np.sum((coef - sibl_coef)**2,axis=0) # = little difference between adjacent leaves. Also gives good results.\n",
    "        ind = np.argsort(sum_coef)\n",
    "        n_prunings = np.floor(coef.shape[1] * self.prune_pct).astype(int)\n",
    "        pruned = 0\n",
    "        i = 0\n",
    "        while pruned < n_prunings:\n",
    "            tree_ind = self.ind_trees_[ind[i]]\n",
    "            leaf_ind = self.ind_leaves_[ind[i]]\n",
    "            res = self.trees_[tree_ind].merge_leaves(leaf_ind)\n",
    "            if res:\n",
    "                pruned += 1\n",
    "            i += 1\n",
    "        to_delete = []\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            if tree.update_leaves():\n",
    "                to_delete.append(tree)\n",
    "        for tree in to_delete:\n",
    "            treeind = self.trees_.index(tree)\n",
    "            del self.rf_.estimators_[treeind]\n",
    "            self.trees_.remove(tree)\n",
    "        self.leaves()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_pruned = 0\n",
    "        while n_pruned <= self.n_prunings:\n",
    "            indicators = self.get_indicators(X)\n",
    "            #print('Model size: {} leaves'.format(indicators.shape[1]))\n",
    "            #self.svr = SVR(C=self.C,fit_intercept=False,epsilon=0.)\n",
    "            self.lr = LR(C=self.C,\n",
    "                            fit_intercept=False,\n",
    "                            solver='lbfgs',\n",
    "                            max_iter=100,\n",
    "                            multi_class='multinomial', n_jobs=-1)\n",
    "            self.lr.fit(indicators,y)\n",
    "            if n_pruned < self.n_prunings:\n",
    "                self.prune_trees()\n",
    "            n_pruned += 1\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            offset = self.offsets_[tree_ind]\n",
    "            tree.value[tree.leaves,0,:] = self.lr.coef_[:,offset:offset + tree.leaves.shape[0]].T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.lr.predict_proba(self.get_indicators(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removestringprefix(string):\n",
    "    return int(string.split(\" \")[1])\n",
    "def load_data():\n",
    "    ##load the data\n",
    "    train = pd.read_csv('data/train.csv',index_col='id',\n",
    "                    converters = {'location':removestringprefix}) \n",
    "    train=train.fillna(0)\n",
    "    test = pd.read_csv('data/test.csv',index_col='id',\n",
    "                    converters = {'location':removestringprefix})\n",
    "    test=test.fillna(0)\n",
    "    event_type = pd.read_csv('data/event_type.csv',\n",
    "                            converters={'event_type':removestringprefix})\n",
    "    log_feature = pd.read_csv('data/log_feature.csv',\n",
    "                             converters={'log_feature':removestringprefix})\n",
    "    resource_type = pd.read_csv('data/resource_type.csv',\n",
    "                               converters={'resource_type':removestringprefix})\n",
    "    severity_type = pd.read_csv('data/severity_type.csv',index_col='id',converters={'severity_type':removestringprefix})\n",
    "    loc = pd.concat((train[['location']],test[['location']]),axis=0)\n",
    "    return dict(train=train,test=test,loc=loc,events=event_type,log=log_feature,res=resource_type,sev=severity_type)\n",
    "def build_features(data):\n",
    "    features = []\n",
    "    sev=data['sev']\n",
    "    loc=data['loc']\n",
    "    events=data['events']\n",
    "    log=data['log']\n",
    "    res=data['res']\n",
    "    ##length of severity and the total dataste is same. so it makes sence to\n",
    "    ##merge severity data and location data as it is a 1:1 mapping\n",
    "    df=pd.DataFrame(0,index=sev.index,columns=[])\n",
    "    df['severity_type']=sev.severity_type\n",
    "    df['fault_severity']=data['train'].fault_severity\n",
    "    df['location']=loc.location\n",
    "    #number the seve within location helps us to figure out which location is critical for worse sevs\n",
    "    df['num']=df.groupby('location')['severity_type'].transform(lambda x:np.arange(x.shape[0])+1)\n",
    "    df['normalizedsevcount']=df.groupby('location')['num'].transform(lambda x:x/(x.max()+1))\n",
    "    loccount=pd.DataFrame(loc['location'].value_counts()).rename(columns={'location':'loc_count'})\n",
    "    df=pd.merge(df,loccount,how='left',left_on='location',right_index=True)\n",
    "    df=df.fillna(0)\n",
    "    ##find out which ids caused the most events. may be this feature would be helpful\n",
    "    event_count=pd.DataFrame(events['id'].value_counts()).rename(columns={'id':'event_count'})\n",
    "    df=pd.merge(df,event_count,how='left',left_index=True,right_index=True)\n",
    "    ## all such events ,logs,resources will be made as columns for the ids. So that we can compare the data better.\n",
    "    eventsfreqperid=events.groupby(['id','event_type'])['id'].count().unstack().fillna(0).add_prefix('event_')\n",
    "    ## the freq of a event for any id ..\n",
    "    df=pd.merge(df,eventsfreqperid,how='left',right_index=True,left_index=True).fillna(0)\n",
    "    ## as per dune dweller, instead of volume , tkae the log of it to have normalised range\n",
    "    log['logvolume'] = np.log(log.volume + 1)\n",
    "    df['volsumlog'] = np.log1p(log.groupby('id')['volume'].agg('sum'))\n",
    "    logvolagg=log.groupby('id')['logvolume'].agg(['count','min','mean','max','std','sum']).fillna(0).add_prefix('logvolume_')\n",
    "    logvol = log.groupby('id')['logvolume'].agg(['count','min','mean','max','std','sum']).fillna(0).add_prefix('logvolume_')\n",
    "    df = pd.merge(df, logvol, how='left', right_index=True, left_index=True).fillna(0)\n",
    "    logvolfreqperid=log.groupby(['id','log_feature'])['logvolume'].mean()\n",
    "    logvolfreqperid=logvolfreqperid.unstack().fillna(0).add_prefix('logfeatvol_').fillna(0)\n",
    "    df=pd.merge(df,logvolfreqperid,how='left',left_index=True,right_index=True)\n",
    "    nresources = pd.DataFrame(res['id'].value_counts()).rename(columns={'id':'nresources'})\n",
    "    df=pd.merge(df,nresources,how='left',left_index=True,right_index=True).fillna(0)\n",
    "    resfreq=res.groupby(['id','resource_type'])['resource_type'].count()\n",
    "    resfreq=resfreq.unstack().fillna(0).add_prefix('resourcetype_')\n",
    "    df=pd.merge(df,resfreq,how='left',left_index=True,right_index=True).fillna(0)\n",
    "    print(\"shape\",df.shape)\n",
    "    return df\n",
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "        true class, intergers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (18552, 464)\n",
      "Random Forest when max features is 40: train loss 0.4877, test loss 0.5203\n",
      "Random Forest when max features is 5: train loss 0.7008, test loss 0.7123\n",
      "Refined Random Forest: train loss 0.2164, test loss 0.4273\n",
      "Most important features:\n",
      "severity_type\n",
      "location\n",
      "num\n",
      "normalizedsevcount\n",
      "loc_count\n",
      "event_count\n",
      "event_11\n",
      "event_13\n",
      "event_15\n",
      "event_20\n",
      "event_34\n",
      "event_35\n",
      "volsumlog\n",
      "logvolume_count\n",
      "logvolume_min\n",
      "logvolume_mean\n",
      "logvolume_max\n",
      "logvolume_std\n",
      "logvolume_sum\n",
      "logfeatvol_54\n",
      "logfeatvol_68\n",
      "logfeatvol_70\n",
      "logfeatvol_71\n",
      "logfeatvol_73\n",
      "logfeatvol_82\n",
      "logfeatvol_134\n",
      "logfeatvol_170\n",
      "logfeatvol_193\n",
      "logfeatvol_195\n",
      "logfeatvol_203\n",
      "logfeatvol_232\n",
      "logfeatvol_233\n",
      "logfeatvol_273\n",
      "logfeatvol_291\n",
      "logfeatvol_312\n",
      "logfeatvol_313\n",
      "logfeatvol_315\n",
      "logfeatvol_368\n",
      "nresources\n",
      "resourcetype_2\n",
      "resourcetype_6\n",
      "resourcetype_8\n",
      "The refined forest and the random forest is overfitting\n"
     ]
    }
   ],
   "source": [
    "data=load_data()\n",
    "df = build_features(data)\n",
    "y=data['train'].fault_severity\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "df.fillna(0)\n",
    "kf = StratifiedKFold(y.values, n_folds=10, shuffle=True, random_state = 1234)\n",
    "for itrain, itest in kf:\n",
    "    clf = RandomForestClassifier(n_estimators=300, min_samples_leaf=15, max_features=45, random_state=1)\n",
    "    clf1 = RandomForestClassifier(n_estimators=300, max_depth=7, max_features=5, random_state=1)\n",
    "    if itrain is None:\n",
    "        # No indices into train, so return all train data\n",
    "            itrain = train.index\n",
    "    else:\n",
    "            itrain = train.index[itrain]\n",
    "    if itest is None:\n",
    "            itest = test.index\n",
    "    else:\n",
    "            itest = train.index[itest]\n",
    "    Xtr=df.loc[itrain]\n",
    "    Xtr = Xtr.drop(['fault_severity'], axis=1)\n",
    "    Xte=df.loc[itest]\n",
    "    Xte = Xte.drop(['fault_severity'], axis=1)\n",
    "    ytr = df.loc[itrain, 'fault_severity']\n",
    "    yte = df.loc[itest,'fault_severity']\n",
    "    ##Xte.drop('fault_severity')\n",
    "    ##Xtr.drop('fault_severity')\n",
    "    clf.fit(Xtr, ytr)\n",
    "    ##print(\"predict train\",clf.predict_proba(Xtr))\n",
    "    ##print(\"predict test\",clf.predict_proba(Xte))\n",
    "    loss2tr = multiclass_log_loss(ytr.values, clf.predict_proba(Xtr))\n",
    "    loss2te = multiclass_log_loss(yte.values, clf.predict_proba(Xte))\n",
    "    print(\"Random Forest when max features is 40: train loss {:.4f}, test loss {:.4f}\".format(loss2tr, loss2te))\n",
    "    clf1.fit(Xtr, ytr)\n",
    "    loss5tr = multiclass_log_loss(ytr.values, clf1.predict_proba(Xtr))\n",
    "    loss5te = multiclass_log_loss(yte.values, clf1.predict_proba(Xte))\n",
    "    print(\"Random Forest when max features is 5: train loss {:.4f}, test loss {:.4f}\".format(loss5tr, loss5te))\n",
    "    rrf = RefinedRandomForest(clf, C = 0.01, n_prunings = 0)\n",
    "    rrf.fit(Xtr, ytr)\n",
    "    loss3tr = multiclass_log_loss(ytr.values, rrf.predict_proba(Xtr))\n",
    "    loss3te = multiclass_log_loss(yte.values, rrf.predict_proba(Xte))\n",
    "    ##print(\"predict train\",rrf.predict_proba(Xtr))\n",
    "    ##print(\"predict test\",rrf.predict_proba(Xte))\n",
    "    print(\"Refined Random Forest: train loss {:.4f}, test loss {:.4f}\".format(loss3tr, loss3te))\n",
    "    important_features = []\n",
    "    print ('Most important features:')\n",
    "    for x,i in enumerate(clf.feature_importances_):\n",
    "        if i>np.average(clf.feature_importances_):\n",
    "            ##important_features.append(str(x))\n",
    "            print(Xtr.columns[x])\n",
    "    break\n",
    "print(\"The refined forest and the random forest is overfitting\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
